{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import snscrape.modules.twitter as scrape_tweets\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Collection of Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def collect_data():\n",
    "    location = '4.4149, -3.0424, 680km'\n",
    "    query = '(\"elevy\" OR \"e-levy\" OR \"increase OR VAT\" OR \"Financial OR sector OR levy\" OR \"income OR tax OR bill\" OR \"covid OR levy\" OR \"sustainability OR levy\" OR \"addo OR levy\" OR \"covid OR tax\" OR \"betting OR tax\" OR \"exercise OR duty OR bill\" OR \"electronic OR levy\") until:2023-06-20 since:2020-03-01 geocode:\"{}\"'.format(location)\n",
    "\n",
    "    tweets = []\n",
    "\n",
    "\n",
    "    for tweet in scrape_tweets.TwitterSearchScraper(query).get_items():\n",
    "        tweets.append([tweet.date, tweet.username, tweet.content])\n",
    "\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(tweets, columns=['Date', 'User', 'Tweet'])\n",
    "    df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing of Tweet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove mentions\n",
    "    menttion_pattern = re.compile(r'@\\w+')\n",
    "    text = re.sub(menttion_pattern, '', text)\n",
    "\n",
    "    # Remove Hashtags\n",
    "    text = re.sub(r'#', '', text)\n",
    "\n",
    "    # Remove retweets\n",
    "    text = re.sub(r'RT[\\s]+', '', text)\n",
    "\n",
    "    # Remove urls\n",
    "    text = re.sub(r'https?:\\/\\/\\S+', '', text)\n",
    "\n",
    "    # Remove emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002600-\\U000027BF\"  # miscellaneous symbols\n",
    "                               u\"\\U0001F910-\\U0001F9FF\"  # faces with accessories\n",
    "                               u\"\\u200d\"  # zero-width joiner\n",
    "                               u\"\\u2600-\\u26FF\\u2700-\\u27BF\"  # additional symbols\n",
    "                               u\"\\u3000-\\u303F\"  # punctuation symbols\n",
    "                               u\"\\uFE0F\"  # emoji variation selector\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text = re.sub(emoji_pattern, '', text)\n",
    "\n",
    "    # Remove newlines\n",
    "    newline_pattern = re.compile(r'\\n')\n",
    "    text = re.sub(newline_pattern, '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "df['Tweet'] = df['Tweet'].apply(clean_text)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sentiment Analysis using Textblob"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read Data\n",
    "df = pd.read_csv('cleaned_tweet_2.csv')\n",
    "\n",
    "tweets = df['Tweet']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentiments = []\n",
    "for tweet in tweets:\n",
    "    blob = TextBlob(str(tweet))  # Convert tweet to string if it's not already\n",
    "    polarity = blob.sentiment.polarity\n",
    "    if polarity > 0:\n",
    "        sentiment = 'Positive'\n",
    "    elif polarity < 0:\n",
    "        sentiment = 'Negative'\n",
    "    else:\n",
    "        sentiment = 'Neutral'\n",
    "    sentiments.append(sentiment)\n",
    "\n",
    "# Add the sentiments to the DataFrame\n",
    "data['Sentiment'] = sentiments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the sentiment distribution\n",
    "sentiment_counts = data['Sentiment'].value_counts()\n",
    "plt.bar(sentiment_counts.index, sentiment_counts.values)\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sentiment Analysis using Vader(NLTK)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run polarity score on entire dataframe\n",
    "res = {}\n",
    "for i, row  in tqdm(df.iterrows(), total=len(df)):\n",
    "    tweet = row['Tweet']\n",
    "    myid = row['User']\n",
    "    res[myid] = sia.polarity_scores(tweet)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vaders = pd.DataFrame(res).T\n",
    "vaders = vaders.reset_index().rename(columns={'index': 'User'})\n",
    "vaders = vaders.merge(df, how='left')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sentiment score and metadata\n",
    "vaders.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read Data\n",
    "df = pd.read_csv('Dataset\\\\cleaned_tweet_2.csv')\n",
    "\n",
    "tweets = df['Tweet']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sentiment Analysis using Roberta"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read Data\n",
    "df = pd.read_csv('Dataset\\\\cleaned_tweet_2.csv')\n",
    "\n",
    "tweets = df['Tweet']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from scipy.special import softmax"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MODEL = f'cardiffnlp/twitter-roberta-base-sentiment'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run on Roberta\n",
    "encoded_text = tokenizer(example, return_tensors='pt')\n",
    "output = model(**encoded_text)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "scores_dict = {\n",
    "    'roberta_neg': scores[0],\n",
    "    'roberta_neu': scores[1],\n",
    "    'roberta_pos': scores[2]\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def polarity_scores_roberta(example):\n",
    "    encoded_text = tokenizer(example, return_tensors='pt')\n",
    "    output = model(**encoded_text)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    scores_dict = {\n",
    "        'roberta_neg': scores[0],\n",
    "        'roberta_neu': scores[1],\n",
    "        'roberta_pos': scores[2]\n",
    "    }\n",
    "    return scores_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res = {}\n",
    "try:\n",
    "    for i, row  in tqdm(df.iterrows(), total=len(df)):\n",
    "        tweet = row['Tweet']\n",
    "        myid = row['User']\n",
    "        vader_result = res[myid] = sia.polarity_scores(tweet)\n",
    "        vader_result_rename = {}\n",
    "        for key, value in vader_result.items():\n",
    "            vader_result_rename[f\"vader_{key}\"] = value\n",
    "\n",
    "        roberta_result = polarity_scores_roberta(tweet)\n",
    "        both = {**vader_result, **roberta_result}\n",
    "        res[myid] = both\n",
    "except RuntimeError:\n",
    "    printf(f'Broke for id {myid}')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
